{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation as a tool for network introspection\n",
    "\n",
    "In this tutorial we will learn how to visualise a trained model. Step-by-step, we are going to: \n",
    "\n",
    "1) Explore the filter structure of a trained network used for scientific applications\n",
    "\n",
    "2) Compare filter structures and develop strategies to get meaningful results\n",
    "\n",
    "3) Visualise activation, or feature maps\n",
    "\n",
    "### Data and trained network model\n",
    "The data are simulated astrophysical 3D lightcones of the 21cm signal of the forbidden spin-flip transition of neutral hydrogen, generated with the semi-numerical code 21cmFAST (https://github.com/21cmfast/21cmFAST), in the case of mock lightcones noise was generated with the code 21cmSense (https://github.com/jpober/21cmSense), to represent data as expected for the Square Kilometre Array at radio wavelengths. The trained network model is the 3D-21cmPIE-Net (see https://github.com/stef-neu/3D-21cmPIE-Net) as published in Neutsch, Heneka, Brueggen 2022 (arXiv:2201.07587). Credit of the code go to Steffen Neutsch, Caroline Heneka 2022. \n",
    "\n",
    "### General remarks\n",
    "As we use trained models for this task, GPU are not necessarily required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some packages that we will need\n",
    "#uncomment on google colab:\n",
    "# !git clone https://github.com/csheneka/introspection-tutorial.git\n",
    "#!pip install tensorflow==2.12\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "# !pip install tf-keras-vis tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional color-scheme for plotting later on\n",
    "import matplotlib\n",
    "from matplotlib import colors\n",
    "\n",
    "eor_colour = colors.LinearSegmentedColormap.from_list(\n",
    "    \"EoR2\",\n",
    "    [\n",
    "        (0, \"white\"),\n",
    "        (0.21, \"yellow\"),\n",
    "        (0.42, \"orange\"),\n",
    "        (0.63, \"red\"),\n",
    "        (0.86, \"black\"),\n",
    "        (0.9, \"blue\"),\n",
    "        (1, \"cyan\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "matplotlib.colormaps.register(name='EoR2', cmap=eor_colour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load a trained model and explore its filter structure\n",
    "\n",
    "The trained model is available both trained on simulations-only and on mock data including noise.\n",
    "\n",
    "### Question 1: \n",
    "What do you notice about the trained model(s)? How is the structure of the model, what is the architecture? How many filters are there to visualise in every layer?\n",
    "\n",
    "### Task 1: \n",
    "Explore plotting the filters of the first layer, how do they look, what might they do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a trained model(s) and get information about its architecture\n",
    "# model 1 simulations-only: models/3D_21cmPIE_Net_sim_par6.h5 (path depends if working on binder or google colab)\n",
    "# model 2 mock: models/3D_21cmPIE_Net_optmock_par6.h5\n",
    "\n",
    "model = keras.models.load_model('')\n",
    "\n",
    "# print out the architecture and find out about the filter / kernel sizes used\n",
    "# model.?\n",
    "# layer.name? filters.shape?\n",
    "# summarize filter shapes\n",
    "\n",
    "# Also: You can re-do (all) tasks with the model trained on mock data ('3D_21cmPIE_Net_optmock_par6.h5') and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: We have a look at the non-averaged filters, e.g. the spatial 2D part of the filters\n",
    "\n",
    "nfilter = \n",
    "nplots = nfilter\n",
    "zpix = \n",
    "n_axis = 6\n",
    "for i in range(nplots):\n",
    "    weights = model.layers[1].weights[0].numpy()\n",
    "    # print(weights.shape)\n",
    "    weights = weights.reshape(?filtershape?,nfilter)[:,:,:,i]\n",
    "    plt.subplot(n_axis, n_axis, 1 + i)\n",
    "    plt.axis('off')\n",
    "    xy = plt.imshow(weights[:,:,zpix],cmap=\"viridis\")\n",
    "    # plt.colorbar(xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Compare filter structures and develop strategies to get meaningful results\n",
    "\n",
    "### Question 2: \n",
    "Could we do something with the filters, knowing the variation in 2D (spatial) is very different from the +1D temporal dimension?\n",
    "\n",
    "### Task 2: \n",
    "Plot each of the nfilters from the first convolutional layer averaged over spatial dimensions. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: (for example) Spatially average and plot filters\n",
    "nfilter = \n",
    "nplots = nfilter\n",
    "n_axis = 6\n",
    "for i in range(nplots):\n",
    "    weights = model.layers[1].weights[0].numpy()\n",
    "    weights = weights.reshape(?filtershape?,nfilter)[:,:,:,i]\n",
    "    weights = np.mean(?) # spatial dims\n",
    "    x = np.linspace(0,len(weights),len(weights))\n",
    "    plt.subplot( n_axis, n_axis, 1 + i )\n",
    "    plt.scatter(x,weights,s=4)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Visualise activation, or feature maps\n",
    "### 3a) Load data from Sample File with (21cm) lighcones at radio wavelengths\n",
    "\n",
    "### Task 3a: \n",
    "Load and plot 3D lightcone data. Explore spatial and temporal structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read light-cones from tfrecords files\n",
    "# to simplify things, we use knowledge on the data shape and input features for parsing\n",
    "HII_DIM=140\n",
    "BOX_LEN=2350\n",
    "nlabel = 6\n",
    "nz = 92\n",
    "def parse_function(files):\n",
    "    keys_to_features = {\"label\":tf.io.FixedLenFeature((nlabel),tf.float32),\n",
    "                        \"image\":tf.io.FixedLenFeature((),tf.string),\n",
    "                        \"tau\":tf.io.FixedLenFeature((),tf.float32),\n",
    "                        \"gxH\":tf.io.FixedLenFeature((nz),tf.float32),\n",
    "                        \"z\":tf.io.FixedLenFeature((nz),tf.float32),}\n",
    "    parsed_features = tf.io.parse_example(files, keys_to_features)\n",
    "    image = tf.io.decode_raw(parsed_features[\"image\"],tf.float16)\n",
    "    image = tf.reshape(image,(HII_DIM,HII_DIM,BOX_LEN))\n",
    "    return image, parsed_features[\"label\"] # Image, label ordering: m_WDM,Omega_m,L_X,E_0,T_vir,zeta\n",
    "\n",
    "data_file=\"data/lc0.tfrecord\" # large file: lc1n.tfrecord\n",
    "#google colab: might have to adjust the path e.g. to 'introspection-tutorial/..'\n",
    "dataset = tf.data.TFRecordDataset(data_file)\n",
    "dataset = dataset.map(parse_function,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "data = np.array(list(dataset.as_numpy_iterator()),dtype=object)\n",
    "print(data.shape)\n",
    "print(data[0,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot lightcone slices\n",
    "N_lcs = \n",
    "print('lighcones in dataset: ',N_lcs)\n",
    "no_lc = 9\n",
    "nslice = 139\n",
    "\n",
    "for i in range(N_lcs):\n",
    "    lc = data[i,0]\n",
    "    lc_slice = \n",
    "    plt.subplot(N_lcs, 1, 1 + i)    \n",
    "    xy = plt.imshow(lc_slice,cmap=\"EoR\",vmin=-150,vmax=30)\n",
    "    # plt.colorbar(xy,orientation=\"horizontal\",aspect=40)\n",
    "    \n",
    "# bonus: for light-cones and activations you can make e.g. impressive-looking gifs of moving through the 3D data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b: \n",
    "Plot the activation or feature maps after the first layer for an example lightcone.\n",
    "### Question 3: \n",
    "How do the activations look like in the first layer? How later on? What do they tell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the activation, or features\n",
    "nslice = \n",
    "nfilter = \n",
    "\n",
    "lc_predict = data[0,0]\n",
    "img_tensor = np.expand_dims(lc_predict, axis=0)\n",
    "\n",
    "# get the activation for example after the first layer\n",
    "activation_model = keras.Model(inputs=model.inputs, outputs=model.layers[1].output)\n",
    "activation = activation_model(img_tensor)\n",
    "print(activation.shape)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(32):\n",
    "    plt.subplot(6,6,i+1)\n",
    "    feature_map = \n",
    "    xy = plt.imshow(feature_map)\n",
    "    plt.colorbar(xy,orientation=\"vertical\",aspect=40)\n",
    "    \n",
    "# alternate approach: \n",
    "# directly multiply and scan the data with the filters to see the effect of convolution with the kernel 'in action'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
